{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://study-ai.com/jdla/\" target=\"_blank\"><img src=\"http://ai999.careers/bnr_jdla.png\" alt=\"[Study-AI]3ヵ月で現場で潰しが効く ディープラーニング講座\" title=\"[Study-AI]3ヵ月で現場で潰しが効く ディープラーニング講座\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: right;\"><time datetime=\"2020-01-23\">令和2年1月23日</time></p>\n",
    "<p style=\"text-align: right;\">北川一樹</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レポート（深層学習・day2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : 勾配消失問題\n",
    "- 出力層から入力層へ向かって勾配が緩やかになって行き、最終的に0になる\n",
    "- 導関数の最大値が小さいシグモイド関数は勾配消失を起こし易い\n",
    "    - シグモイド関数$\\sigma(\\cdot)$\n",
    "    ```python\n",
    "    # シグモイド関数（ロジスティック関数）\n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    ```\n",
    "$$\n",
    "\\sigma(u) = \\frac{1}{1 + \\exp(-u)}\n",
    "$$\n",
    "    - $\\sigma(\\cdot)$の導関数\n",
    "    ```python\n",
    "    # シグモイド関数（ロジスティック関数）の導関数\n",
    "    def d_sigmoid(x):\n",
    "        dx = (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "        return dx\n",
    "    ```\n",
    "$$\n",
    "\\sigma(u)' = (1-\\sigma(u))\\cdot\\sigma(u)\n",
    "$$\n",
    "        - $u=0$のとき\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sigma(0)' &=& (1-\\sigma(0))\\cdot\\sigma(0) \\\\\n",
    "&=&(1-0.5)\\cdot0.5 \\\\\n",
    "&=&0.25\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "        - 最大値は$0.25$\n",
    "- 解決策\n",
    "    - 活性化関数の選択\n",
    "        - ReLU関数\n",
    "        ```python\n",
    "        # ReLU関数\n",
    "        def relu(x):\n",
    "            return np.maximum(0, x)\n",
    "        ```\n",
    "$$\n",
    "f(x)\n",
    "=\n",
    "\\begin{cases}\n",
    "x & (x > 0) \\\\\n",
    "0 & (x \\leq 0)\n",
    "\\end{cases}\n",
    "$$\n",
    "            - 勾配消失問題とスパース化問題を回避\n",
    "    - 重みの初期設定\n",
    "        - ゼロ\n",
    "            - 全ての値が同じ値で伝わる為、パラメータのチューニングが行われなくなる\n",
    "        - Xavier（ザビエル）\n",
    "        ```python\n",
    "        # Xavierの初期値\n",
    "        network['W1'] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size)\n",
    "        network['W2'] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size)\n",
    "        ```\n",
    "        - He\n",
    "        ```python\n",
    "        # Heの初期値\n",
    "        network['W1'] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
    "        network['W2'] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
    "        ```\n",
    "            - ReLU関数と付随\n",
    "    - バッチ正規化\n",
    "        - ミニバッチ単位で、入力値のデータの偏りを抑制する手法\n",
    "        - 総入力$\\mathbf{u}$又は中間層出力$\\mathbf{z}$で行われる\n",
    "        - 一時的な効果\n",
    "            - 計算の高速化\n",
    "            - 勾配消失が起きづらくなる\n",
    "\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 : 学習率最適化手法\n",
    "- 決め方\n",
    "    - 大きめの値から徐々に小さくして行く\n",
    "- パラメータ更新\n",
    "    - 通常の手法\n",
    "        - 勾配降下法\n",
    "            - 誤差をパラメータで微分したものと学習率の積を減算する\n",
    "    - 学習率最適化手法\n",
    "        - モメンタム\n",
    "        ```python\n",
    "        v[key] = momentum * v[key] - learning_rate * grad[key]\n",
    "        network.params[key] += v[key]\n",
    "        ```\n",
    "            - 誤差のパラメータで微分したものと学習率の積を計算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する\n",
    "            - メリット\n",
    "                - 局所的最適解にはならず、大域的最適解となる\n",
    "                - **谷間に着いてから最も低い位置（最適値）に行く迄の時間が早い**\n",
    "        - AdaGrad\n",
    "        ```python\n",
    "        # h[key]の中に0がないことが前提\n",
    "        h[key] += np.square(grad[key])\n",
    "        network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]))\n",
    "        ```\n",
    "            - 誤差をパラメータで微分したものと再定義した学習率の積を減算する\n",
    "            - メリット\n",
    "                - **勾配のゆるやかな斜面に対して、最適値に近づける**\n",
    "            - デメリット\n",
    "                - 学習率を小さくして行くので鞍点問題を起こし易い\n",
    "                    - 解決策としてRMSPropがある\n",
    "        - RMSProp\n",
    "        ```python\n",
    "        h[key] *= decay_rate\n",
    "        h[key] += (1 - decay_rate) * np.square(grad[key])\n",
    "        network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]) + 1e-7)\n",
    "        ```\n",
    "            - 誤差をパラメータで微分したものと再定義した学習率の積を減算する**AdaGradの改良版**\n",
    "            - メリット\n",
    "                - 局所的最適解にはならず、大域的最適解となる\n",
    "                - **ハイパーパラメータの調整が必要な場合が少ない**\n",
    "        - Adam\n",
    "            ```python\n",
    "            m[key] += (1 - beta1) * (grad[key] - m[key])\n",
    "            v[key] += (1 - beta2) * (grad[key] ** 2 - v[key])\n",
    "            network.params[key] -= learning_rate_t * m[key] / (np.sqrt(v[key]) + 1e-7) \n",
    "            ```\n",
    "            - モメンタムの、過去の勾配の指数関数的減衰平均\n",
    "            - RMSPropの、過去の勾配の2乗の指数関数的減衰平均\n",
    "            - モメンタムとRMSPropとの最適化アルゴリズム\n",
    "            - メリット\n",
    "                - **モメンタム及びRMSPropのメリットを孕んだ**アルゴリズム\n",
    "                    - 故に最も多用される最適化手法\n",
    "\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 : 過学習\n",
    "# 過学習\n",
    "- 原因\n",
    "    - パラメータの数が多い\n",
    "    - パラメータの値が適切ではない\n",
    "        - 重みの値が大きい\n",
    "    - ノード数が多い etc...\n",
    "- 解決策\n",
    "    - 正則化\n",
    "        - ネットワークの自由度（層数、ノード数、パラメータの値）を制限すること\n",
    "        - 手法\n",
    "            - L1正則化$(P=1)$\n",
    "                - Lasso\n",
    "                - スパース化問題に対応\n",
    "                - 符号関数を使う\n",
    "                - `weight_decay_lambda`\n",
    "                    - 値が小さ過ぎると過学習が起き易くなる\n",
    "                    - 値が大き過ぎると学習が進まなくなる\n",
    "            - L2正則化$(P=2)$\n",
    "                - Ridge\n",
    "                - ハイパーパラメータを大きな値にすると全ての重みが限りなく0に近付く\n",
    "            - ドロップアウト\n",
    "                - ランダムにノードを非活性にして学習\n",
    "                - CNNでよく使われる\n",
    "            - Weight Decay (荷重減衰)\n",
    "                - 過学習が起きそうな重みの大きさ以下に抑え、かつ重みの大きさにばらつきを出す\n",
    "\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 : 畳み込みニューラルネットワークの概念\n",
    "- LeNet\n",
    "- 入力層\n",
    "    - 多次元データの平坦化（flatten）が不要\n",
    "    - 画像\n",
    "    - 音楽\n",
    "- 畳み込み層\n",
    "    - 構成\n",
    "    1. 入力値\n",
    "    1. フィルター\n",
    "        - バイアス\n",
    "        - パディング\n",
    "        - ストライド\n",
    "        - チャンネル\n",
    "            - 奥行き数\n",
    "            - フィルターはチャンネル数分用意\n",
    "    1. 出力値\n",
    "    1. バイアス\n",
    "    1. 活性化関数\n",
    "    1. 出力値\n",
    "- プーリング層\n",
    "   - 手法\n",
    "       - マックスプーリング\n",
    "       - アベレージプーリング\n",
    "   - 特徴マップのサイズ（の公式）\n",
    "       - ストライド$$\\mathrm{S}$$\n",
    "       - パディング$$\\mathrm{P}$$\n",
    "       - フィルターの高さ$$\\mathrm{FH}$$\n",
    "       - フィルターの幅$$\\mathrm{FW}$$\n",
    "       - 入力画像の高さ$$\\mathrm{H}$$\n",
    "       - 入力画像の幅$$\\mathrm{W}$$\n",
    "       - 特徴マップの高さ\n",
    "$$\n",
    "\\mathrm{OH}=\\frac{\\mathrm{H}+2\\mathrm{P}-\\mathrm{FH}}{\\mathrm{S}}+1\n",
    "$$\n",
    "       - 特徴マップの幅\n",
    "$$\n",
    "\\mathrm{OW}=\\frac{\\mathrm{W}+2\\mathrm{P}-\\mathrm{FW}}{\\mathrm{S}}+1\n",
    "$$\n",
    "- 全結合層\n",
    "    - 一次元データを扱う\n",
    "\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|明松真司, 田原眞一（杉山将監修）|徹底攻略 ディープラーニングG検定 ジェネラリスト問題集|インプレス|2019年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5 : 最新のCNN\n",
    "- AlexNet\n",
    "    - 画像認識コンペティションILSVRC2012で優勝したモデル\n",
    "    - 5層の畳み込み層・プーリング層、3層の全結合層で構成\n",
    "    - サイズ4096の全結合層にドロップアウトを使用\n",
    "\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レポート（深層学習・day3）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : 再帰型ニューラルネットワークの概念\n",
    "- RNNと略される\n",
    "- **時系列データ**に対応可能なニューラルネットワーク\n",
    "    - 時系列データとは、時間的順序を追って一定間隔毎に観察され、しかも相互に統計的依存関係が認められるようなデータの系列\n",
    "    - 時間的順序を保持したデータの系列\n",
    "        - Pythonのデータ構造でいえば、順序を保持した「リスト」、順序を保持しない「集合」の中では前者に近い\n",
    "    - 例. 音声データ、テキストデータ\n",
    "- 中間層が重要\n",
    "    - 各時刻の状態を区別する為、初期の時刻を$0$、最新の時刻を$4$としたとき$$\\mathrm{s}_{0},\\mathrm{s}_{1},\\mathrm{s}_{3}, \\mathrm{s}_{4}$$と列挙できる\n",
    "        - $\\mathrm{s}_{0}$は初期の状態、$\\mathrm{s}_{4}$は最新の状態を表わす\n",
    "        - 任意の時刻$t$の状態は$$s_{t}$$と表わす\n",
    "- RNNの全体像\n",
    "$$\n",
    "\\begin{array}{rrrrr}\n",
    "    \\mathbf{x}_{4} & \\underrightarrow{\\hspace{16pt}\\mathbf{W}_{\\mathrm{(in)}}\\hspace{16pt}} & \\mathrm{s}_{4} & \\underrightarrow{\\hspace{14pt}\\mathbf{W}_{\\mathrm{(out)}}\\hspace{14pt}} & \\mathbf{y}_{4} \\\\\n",
    "    && \\mathbf{W}\\uparrow && \\\\\n",
    "    \\mathbf{x}_{3} & \\underrightarrow{\\hspace{16pt}\\mathbf{W}_{\\mathrm{(in)}}\\hspace{16pt}} & \\mathrm{s}_{3} & \\underrightarrow{\\hspace{14pt}\\mathbf{W}_{\\mathrm{(out)}}\\hspace{14pt}} & \\mathbf{y}_{3} \\\\\n",
    "    && \\mathbf{W}\\uparrow && \\\\\n",
    "    \\mathbf{x}_{2} & \\underrightarrow{\\hspace{16pt}\\mathbf{W}_{\\mathrm{(in)}}\\hspace{16pt}} & \\mathrm{s}_{2} & \\underrightarrow{\\hspace{14pt}\\mathbf{W}_{\\mathrm{(out)}}\\hspace{14pt}} & \\mathbf{y}_{2} \\\\\n",
    "    && \\mathbf{W}\\uparrow && \\\\\n",
    "    \\mathbf{x}_{1} & \\underrightarrow{\\hspace{16pt}\\mathbf{W}_{\\mathrm{(in)}}\\hspace{16pt}} & \\mathrm{s}_{1} & \\underrightarrow{\\hspace{14pt}\\mathbf{W}_{\\mathrm{(out)}}\\hspace{14pt}} & \\mathbf{y}_{1} \\\\\n",
    "    && \\mathbf{W}\\uparrow && \\\\\n",
    "    && \\mathrm{s}_{0} && \\\\\n",
    "    && \\text{unfold}\\,\\Uparrow && \\\\\n",
    "    \\mathbf{x}_{\\,} & \\underrightarrow{\\hspace1in} & \\mathrm{s}_{\\,} & \\underleftarrow{\\hspace1in} & \\mathbf{y}_{\\,} \\\\\n",
    "    && \\circlearrowright &&\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- 数式\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\mathbf{u}^{t}&=&\\mathrm{W}_{\\mathrm{(in)}}\\mathbf{x}^{t}+\\mathbf{W}\\mathbf{z}^{t-1}+\\mathbf{b} \\tag{} \\\\\n",
    "    \\mathbf{z}^{t}&=&f\\left(\\mathrm{W}_{\\mathrm{(in)}}\\mathbf{x}^{t}+\\mathbf{W}\\mathbf{z}^{t-1}+\\mathbf{b}\\right) \\tag{} \\\\\n",
    "    \\mathbf{v}^{t}&=&\\mathrm{W}_{\\mathrm{(out)}}\\mathbf{z}^{t}+\\mathbf{c} \\tag{} \\\\\n",
    "    \\mathbf{y}^{t}&=&g\\left(\\mathrm{W}_{\\mathrm{(out)}}\\mathbf{z}^{t}+\\mathbf{c}\\right) \\tag{}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- 当該コード（配布ファイル「3_1_simple_RNN.ipynb」80～83行目、バイアス$\\mathbf{b}$等は省略）\n",
    "\n",
    "```python\n",
    "u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
    "z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
    "\n",
    "y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
    "```\n",
    "\n",
    "- RNNの特徴\n",
    "    - 時系列モデルを扱うには、初期の状態$\\mathrm{s}_{0}$と、過去の時間の状態$\\mathrm{s}_{t-1}$を保持し、そこから次の時間での$t$を再帰的に求める再帰構造が必要になる\n",
    "\n",
    "\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "- 問1. サイズ$5 \\times 5$の入力画像を、サイズ$3 \\times 3$のフィルターで畳み込んだ時の出力画像のサイズを答えよ。なおストライドは$2$、パディングは$1$とする。\n",
    "- 答1.\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\mathrm{P}&=&1 \\tag{} \\\\\n",
    "    \\mathrm{S}&=&2 \\tag{} \\\\\n",
    "    \\mathrm{W}&=&\\mathrm{H}&=&5 \\tag{} \\\\\n",
    "    \\mathrm{FW}&=&\\mathrm{FH}&=&3 \\tag{} \\\\\n",
    "    \\mathrm{OW}&=&\\mathrm{OH}&=&\\frac{\\mathrm{H}+2\\mathrm{P}-\\mathrm{FH}}{\\mathrm{S}}+1 \\tag{} \\\\\n",
    "    &&&=&\\frac{5+2\\cdot 1-3}{2}+1 \\tag{} \\\\\n",
    "    &&&=&3 \\tag{}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- 問2. RNNネットワークには大きくわけて3つの重みがある。1つは入力から現在の中間層を定義する際にかけられる重み$\\mathrm{W}_{\\mathrm{(in)}}$、1つは中間層から出力を定義する際にかけられる重み$\\mathrm{W}_{\\mathrm{(out)}}$である。残り1つの重み$\\mathrm{W}$について説明せよ。\n",
    "- 答2. $\\mathrm{W}$は中間層から次の中間層に渡されるときの重みである。\n",
    "\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 : LSTM\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 : GRU\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 : 双方向RNN\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5 : Seq2Seq\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section6 : Word2vec\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section7 : Attention Mechanism\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レポート（深層学習・day4）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : Tensorflowの実装演習\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|\n",
    "|Antonio Gulli, Sujit Pal（大串正矢訳） |直感 Deep Learning ―Python×Kerasでアイデアを形にするレシピ|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 : 強化学習\n",
    "### 実装演習結果\n",
    "### 確認テスト考察\n",
    "### 参考図書\n",
    "|著者名|書籍名|出版社|発行年|\n",
    "|:--|:--|:--|--:|\n",
    "|Guido van Rossum（鴨澤眞夫訳）|Pythonチュートリアル 第3版|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装|オライリー・ジャパン|2016年|\n",
    "|斎藤康毅|ゼロから作るDeep Learning ❷ ―自然言語処理編|オライリー・ジャパン|2018年|\n",
    "|Antonio Gulli, Sujit Pal（大串正矢訳） |直感 Deep Learning ―Python×Kerasでアイデアを形にするレシピ|オライリー・ジャパン|2018年|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
